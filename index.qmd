# LangChain ü¶úüîó

```{python}
#| echo: false
from rich import print
``` 

::: footer
[LangChain dokumentasjon](https://python.langchain.com/docs/introduction/)
:::

## Hva er det?

> LangChain is a framework for developing applications powered by large language
> models.

. . .

- Integrasjon mellom tilbydere av spr√•kmodeller
- Gj√∏r ikke noe beregning selv
	- Alt gj√∏res av eksterne tjenester

## Hvorfor

- _Mange_ tilbydere
- "Spr√•k" for √• kjedesammen elementer til spr√•kmodellene
- Abstraksjon av hendige funksjoner

### Bygge ledetekst

```{python}
from langchain_core.prompts import ChatPromptTemplate

template = "Translate the following into {language}:"
prompt = ChatPromptTemplate.from_messages(
	[("system", template), ("user", "{text}")]
)
```

### Bruke ledetekst

```{python}
specified = prompt.invoke({"language": "Swedish", "text": "Hi everybody!"})
print(specified.to_messages())
```

### Opprette spr√•kmodell

::: {.panel-tabset}
#### Vertex AI

```{python}
from langchain_google_vertexai import ChatVertexAI

llm = ChatVertexAI(model_name="gemini-1.5-flash-002")
```

#### OpenAI

```{.python}
import os

from langchain_openai import AzureChatOpenAI

llm = AzureChatOpenAI(
    azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
    azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
    openai_api_version=os.environ["AZURE_OPENAI_API_VERSION"],
)
```
:::

::: footer
- [Referanse til `Gemini 1.5 Flash` p√• GCP](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-1.5-flash-002)
- [Referanse til `ChatVertexAI` hos LangChain](https://python.langchain.com/docs/integrations/chat/google_vertex_ai_palm/)
- [Referanse til `AzureChatOpenAI` hos LangChain](https://python.langchain.com/docs/integrations/chat/azure_chat_openai/)
:::

### Bruke spr√•kmodellen

```{python}
answer = llm.invoke(specified.to_messages())
print(answer)
```

### Kjedesammen elementer

```{python}
from langchain_core.output_parsers import StrOutputParser

# Opprette en kjede som forenkler √• sette sammen ledetekst, spr√•kmodell og tolkning av svaret
chain = prompt | llm | StrOutputParser()
# All bruk av LangChain fungerer p√• samme m√•te:
chain.invoke(
	{"language": "Icelandic", "text": "Hi to every Data Scientist!"}
)
```

### Abstraksjon

```{.python}
# Kall p√• kjeden og produser output
chain.invoke({...})
# Str√∏m svaret mens det blir produsert
chain.stream({...})
# Hent flere svar samtidig for raskere prosessering
chain.batch([{...}, {...}])
# Finnes ogs√• som asynkrone metoder hvis det trengs
chain.ainvoke({...})
```

### Strukturerte svar

```{python}
from typing import Literal
from pydantic import BaseModel, Field

class Joke(BaseModel):
	"""Setup and punchline of a joke"""
	setup: str = Field(description="The setup of the joke")
	punchline: str = Field(description="The punchline of the joke")
	rating: int = Field(description="A rating of 1-10 of the dad-joke level")

structured_llm = llm.with_structured_output(Joke)
joke_prompt = ChatPromptTemplate.from_messages(
	[("system", "You are a expert level dad-joke teller!"),
	 ("human", "Tell a joke about {subject}")]
)
joke_chain = joke_prompt | structured_llm
print(joke_chain.invoke({"subject": "computers"}))
```

### Bakdelen

- Fint n√•r oppsettet er `ledetekst -> spr√•kmodell -> output`
- Ved mer kompliserte kjeder s√• blir det vanskelig
	- M√• bruke mange spesialiserte `Runnable`-er
	- Skriver ikke vanlig Python

# LangGraph ü¶úüï∏Ô∏è

::: footer
[LangGraph dokumentasjon](https://langchain-ai.github.io/langgraph/)
:::

## Hva er det?
 
 > Build robust and stateful multi-actor applications with LLMs by modeling
 > steps as edges and nodes in a graph.

 . . .

- Enklere √• koble sammen ulike kjeder
	- Bruker vanlige Python metoder!
- Kan ha sykluser

. . .

- M√• ikke v√¶re `multi-actor` üòÖ

## Motiverende eksempel

- Vi √∏nsker √• hente dokumenter fra en database
- N√•r vi f√•r en chat historikk trenger vi kontekst
	- `Hva er dagpenger?`
	- _Svar fra spr√•kmodell_
	- `Kan det samordnes med sykepenger?`

. . .

- Be spr√•kmodellen skrive om sp√∏rsm√•let slik at vi ikke trenger hele chat historikken!
	- `Kan dagpenger samordnes med sykepenger?`

### LangChain implementasjon

```{python}
from langchain_core.prompts import MessagesPlaceholder

contextualize_q_system_prompt = (
    "Given a chat history and the latest user question "
    "which might reference context in the chat history, "
    "formulate a standalone question which can be understood "
    "without the chat history. Do NOT answer the question, "
    "just reformulate it if needed and otherwise return it as is."
)

contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
```

### LangChain implementasjon

```{.python}
from langchain.chains import create_history_aware_retriever

history_aware_retriever = create_history_aware_retriever(
	llm, retriever, contextualize_q_prompt
)
```

### LangChain implementasjon

```{python}
from langchain.chains.combine_documents import create_stuff_documents_chain

system_prompt = (
    "You are an assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer "
    "the question. If you don't know the answer, say that you "
    "don't know. Use three sentences maximum and keep the "
    "answer concise."
    "\n\n"
    "{context}"
)
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
```

### LangChain implementasjon

```{.python}
from langchain.chains import create_retrieval_chain

rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
```

### LangChain implementasjon

```{.python}
rag_chain.invoke(
	{
		"chat_history": [...],
		"input": "Kan det samordnes med sykepenger?",
	}
)
```

### Utfordringen med LangChain

- Vanskelig √• gj√∏re endringer i `Runnable`-ene
- `Runnable` er fint som et brukergrensesnitt til `invoke`, `batch`, mf.
	- Tungvindt √• lage `Runnable` som f.eks. ikke skriver om hvis `chat_history` er tom

### LangGraph implementasjon

```{python}
context_chain = contextualize_q_prompt | llm | StrOutputParser()

def context(state):
	rewritten = context_chain.invoke({"chat_history": state["chat_history"], "input": state["question"]})
	state["rewritten_question"] = rewritten
	return state
```

### LangGraph implementasjon

```{python}
retriever = lambda q: ["Vet ikke...", "Aner ikke!", "Hvorfor sp√∏r du meg?!"]

def retrieve(state):
	docs = retriever(state["rewritten_question"])
	state["context"] = docs
	return state
```

### LangGraph implementasjon

```{python}
generator = qa_prompt | llm | StrOutputParser()


def generate(state):
    answer = generator.invoke({"chat_history": state["chat_history"], "input": state["question"], "context": state["context"]})
    state["answer"] = answer
    return state
```

### LangGraph implementasjon

```{python}
from typing import TypedDict

class GraphState(TypedDict):
    question: str
    chat_history: list[str]
    rewritten_question: str
    context: list[str]
    answer: str
```

### LangGraph implementasjon

```{python}
from langgraph.graph import END, START, StateGraph

workflow = StateGraph(GraphState)

workflow.add_node("contextualize", context)
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate)

workflow.add_edge(START, "contextualize")
workflow.add_edge("contextualize", "retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)

graph = workflow.compile()
```

### LangGraph implementasjon

:::: {.columns}
::: {.column width="40%"}
```{mermaid}
%%| echo: false
%%| eval: true 
%%{init: {'flowchart': {'curve': 'linear'}}}%%
graph TD;
    __start__([<p>__start__</p>]):::first
    contextualize(contextualize)
    retrieve(retrieve)
    generate(generate)
    __end__([<p>__end__</p>]):::last
    __start__ --> contextualize;
    contextualize --> retrieve;
    generate --> __end__;
    retrieve --> generate;
    classDef default fill:#f2f0ff,line-height:1.2
    classDef first fill-opacity:0
    classDef last fill:#bfb6fc
```
:::

::: {.column width="60%"}
::: {layout="[[-1], [1], [-1]]"}
```{.python}
graph.invoke(
	{
		"question": "Kan det samordnes med sykepenger?",
		"chat_history": [...],
	}
)
```
:::
:::
::::

### Endre LangGraph

Hva hvis vi √∏nsker √• _ikke_ omskrive sp√∏rsm√•l hvis det ikke er noe historikk?

. . .

```{.python}
def context(state):
	if len(state["chat_history"]) > 0:
		rewritten = context_chain.invoke({"chat_history": state["chat_history"], "input": state["question"]})
	else:
		rewritten = state["question"]
	state["rewritten_question"] = rewritten
	return state
```

## Hva er mulig med LangGraph?

![](./images/self-rag.png)

::: footer
[Eksempel hentet fra LangGraph](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag/)
:::

# Sp√∏rsm√•l? 
